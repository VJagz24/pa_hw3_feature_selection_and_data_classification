# -*- coding: utf-8 -*-
"""Vedant Jagtap_pa_hw3_feature_selection_and_data_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjdMIUfBA0RgYyqp4pfd0fxOfYuwYu5r
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import spearmanr
import time
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, f_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
import tensorflow as tf
from scipy.stats import randint, uniform
from sklearn.model_selection import RandomizedSearchCV

class KNNFromScratch:
    def __init__(self, k=3):
        self.k = k
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        self.X_train = np.array(X, dtype=np.float64)
        self.y_train = np.array(y, dtype=np.int64)

        print("\nTraining data types:")
        print(f"X_train dtype: {self.X_train.dtype}")
        print(f"y_train dtype: {self.y_train.dtype}")
        print(f"X_train shape: {self.X_train.shape}")
        print(f"y_train shape: {self.y_train.shape}")
        return self

    def euclidean_distance(self, x1, x2):
        x1 = np.array(x1, dtype=np.float64)
        x2 = np.array(x2, dtype=np.float64)
        if not isinstance(x1[0], (int, float)) or not isinstance(x2[0], (int, float)):
            print(f"\nProblem with data types:")
            print(f"x1 type: {type(x1[0])}, value: {x1[0]}")
            print(f"x2 type: {type(x2[0])}, value: {x2[0]}")
            print(f"x1 dtype: {x1.dtype}")
            print(f"x2 dtype: {x2.dtype}")

        try:
            return np.sqrt(np.sum((x1 - x2) ** 2))
        except TypeError as e:
            print(f"\nError in euclidean_distance:")
            print(f"x1: {x1}")
            print(f"x2: {x2}")
            raise e

    def predict_single(self, x):
        x = np.array(x, dtype=np.float64)

        try:
            distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]

            k_indices = np.argsort(distances)[:self.k]

            k_nearest_labels = self.y_train[k_indices]

            return np.bincount(k_nearest_labels).argmax()
        except Exception as e:
            print(f"\nError in predict_single:")
            print(f"Input x shape: {x.shape}")
            print(f"Input x dtype: {x.dtype}")
            print(f"First few values of x: {x[:5]}")
            raise e

    def predict(self, X):
        X = np.array(X, dtype=np.float64)
        print("\nPrediction data info:")
        print(f"X shape: {X.shape}")
        print(f"X dtype: {X.dtype}")

        try:
            return np.array([self.predict_single(x) for x in X])
        except Exception as e:
            print(f"\nError in predict:")
            print(f"First row of X: {X[0]}")
            print(f"X shape: {X.shape}")
            raise e

def run_complete_pipeline(data_path, target_column='Status'):
    print(f"Loading data from {data_path}...")
    df = pd.read_csv(data_path)

    print("\nInitial data info:")
    print(df.info())
    print("\nSample of raw data:")
    print(df.head())

    preprocessor = DataPreprocessor()
    df_processed = preprocessor.preprocess_data(df)
    df_processed = df_processed.astype(np.float64)
    print("\nProcessed data info:")
    print(df_processed.info())
    print("\nSample of processed data:")
    print(df_processed.head())

    X = df_processed.drop(columns=[target_column]).astype(np.float64)
    y = df_processed[target_column].astype(np.int64)

    print("\nFeature matrix info:")
    print(f"X shape: {X.shape}")
    print(f"X dtypes:\n{X.dtypes}")
    print("\nTarget variable info:")
    print(f"y shape: {y.shape}")
    print(f"y dtype: {y.dtype}")
    print(f"Unique values in y: {np.unique(y)}")

    print("\nPerforming feature selection...")
    fs = FeatureSelection(X, y)
    X_corr = fs.correlation_filter()
    X_mi, mi_features = fs.mutual_information()
    X_chi, chi_features = fs.chi_square_selection()
    X_f, f_features = fs.f_score_selection()

    common_features = list(set(mi_features) & set(chi_features) & set(f_features))
    X_selected = X[common_features].astype(np.float64)

    print(f"\nSelected features info:")
    print(f"Number of features: {len(common_features)}")
    print(f"Feature names: {common_features}")
    print(f"X_selected shape: {X_selected.shape}")
    print(f"X_selected dtypes:\n{X_selected.dtypes}")
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42, stratify=y
    )

    X_train = np.array(X_train, dtype=np.float64)
    X_test = np.array(X_test, dtype=np.float64)
    y_train = np.array(y_train, dtype=np.int64)
    y_test = np.array(y_test, dtype=np.int64)

    print("\nTraining/Testing data info:")
    print(f"X_train shape: {X_train.shape}, dtype: {X_train.dtype}")
    print(f"X_test shape: {X_test.shape}, dtype: {X_test.dtype}")
    print(f"y_train shape: {y_train.shape}, dtype: {y_train.dtype}")
    print(f"y_test shape: {y_test.shape}, dtype: {y_test.dtype}")
    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)

    try:
        nn_model, nn_history, nn_metrics = neural_network_tensorflow(X_train, X_test, y_train, y_test)
        results['TensorFlow Neural Network'] = nn_metrics
    except Exception as e:
        print(f"Error in neural network training: {str(e)}")

    return {
        'feature_scores': fs.feature_scores,
        'selected_features': common_features,
        'model_results': results
    }

class DataPreprocessor:
    def __init__(self):
        self.label_encoders = {}
        self.numeric_imputer = SimpleImputer(strategy='mean')
        self.categorical_imputer = SimpleImputer(strategy='most_frequent')
        self.scaler = StandardScaler()

    def encode_categorical_variables(self, df):
        df = df.copy()
        categorical_columns = df.select_dtypes(include=['object']).columns

        for column in categorical_columns:
            if column not in self.label_encoders:
                self.label_encoders[column] = LabelEncoder()
            df[column] = df[column].fillna('missing')
            df[column] = self.label_encoders[column].fit_transform(df[column].astype(str))
            df[column] = df[column].astype(float)

        return df

    def handle_missing_values(self, df):
        df = df.copy()
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

        if len(numeric_columns) > 0:
            df[numeric_columns] = self.numeric_imputer.fit_transform(df[numeric_columns])

        return df

    def detect_and_handle_outliers(self, df, threshold=3):
        df = df.copy()
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

        for column in numeric_columns:
            z_scores = np.abs(stats.zscore(df[column], nan_policy='omit'))
            df.loc[z_scores > threshold, column] = np.nan
            median_val = df[column].median()
            df.loc[df[column].isna(), column] = median_val

        return df

    def normalize_data(self, df):
        df = df.copy()
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

        if len(numeric_columns) > 0:
            df[numeric_columns] = self.scaler.fit_transform(df[numeric_columns])

        return df

    def preprocess_data(self, df, handle_outliers=True):
        print("Starting preprocessing pipeline...")
        df_processed = df.copy()
        for column in df_processed.columns:
            try:
                if df_processed[column].dtype == 'object':
                    if df_processed[column].str.match(r'^-?\d*\.?\d*$').all():
                        df_processed[column] = pd.to_numeric(df_processed[column], errors='coerce')
            except:
                continue
        df_processed = self.encode_categorical_variables(df_processed)
        df_processed = self.handle_missing_values(df_processed)
        if handle_outliers:
            df_processed = self.detect_and_handle_outliers(df_processed)
        df_processed = self.normalize_data(df_processed)
        for column in df_processed.columns:
            if not np.issubdtype(df_processed[column].dtype, np.number):
                print(f"Converting {column} to numeric...")
                df_processed[column] = pd.to_numeric(df_processed[column], errors='coerce')
                df_processed[column].fillna(df_processed[column].mean(), inplace=True)
        df_processed = df_processed.astype(float)

        return df_processed

def run_complete_pipeline(data_path, target_column='Status'):
    print(f"Loading data from {data_path}...")
    df = pd.read_csv(data_path)

    print("\nInitial data info:")
    print(df.info())
    print("\nTarget variable statistics:")
    print(df[target_column].value_counts())
    preprocessor = DataPreprocessor()
    df_processed = preprocessor.preprocess_data(df)
    print("\nChecking data types after preprocessing:")
    print(df_processed.dtypes)
    X = df_processed.drop(columns=[target_column])
    y = df_processed[target_column]
    if not np.issubdtype(y.dtype, np.number):
        le = LabelEncoder()
        y = le.fit_transform(y)
    y = y.astype(int)
    print("\nPerforming feature selection...")
    fs = FeatureSelection(X, y)
    X_corr = fs.correlation_filter()
    X_mi, mi_features = fs.mutual_information()
    X_chi, chi_features = fs.chi_square_selection()
    X_f, f_features = fs.f_score_selection()

    common_features = list(set(mi_features) & set(chi_features) & set(f_features))
    X_selected = X[common_features]
    X_selected = X_selected.astype(np.float64)

    print(f"\nSelected {len(common_features)} features: {common_features}")

    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nTraining data shape: {X_train.shape}")
    print(f"Testing data shape: {X_test.shape}")

    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)

    try:
        nn_model, nn_history, nn_metrics = neural_network_tensorflow(X_train, X_test, y_train, y_test)
        results['TensorFlow Neural Network'] = nn_metrics
    except Exception as e:
        print(f"Error in neural network training: {str(e)}")

    return {
        'feature_scores': fs.feature_scores,
        'selected_features': common_features,
        'model_results': results
    }

class FeatureSelection:
    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.feature_scores = {}

    def correlation_filter(self, threshold=0.7):
        corr_matrix = self.X.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
        return self.X.drop(columns=to_drop)

    def mutual_information(self, k=10):
        mi_scores = mutual_info_classif(self.X, self.y)
        mi_selector = SelectKBest(mutual_info_classif, k=k)
        X_selected = mi_selector.fit_transform(self.X, self.y)
        selected_features = self.X.columns[mi_selector.get_support()].tolist()
        self.feature_scores['mutual_info'] = dict(zip(self.X.columns, mi_scores))
        return X_selected, selected_features

    def chi_square_selection(self, k=10):
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(self.X)
        chi_selector = SelectKBest(chi2, k=k)
        X_selected = chi_selector.fit_transform(X_scaled, self.y)
        selected_features = self.X.columns[chi_selector.get_support()].tolist()
        self.feature_scores['chi2'] = dict(zip(self.X.columns, chi_selector.scores_))
        return X_selected, selected_features

    def f_score_selection(self, k=10):
        f_selector = SelectKBest(f_classif, k=k)
        X_selected = f_selector.fit_transform(self.X, self.y)
        selected_features = self.X.columns[f_selector.get_support()].tolist()
        self.feature_scores['f_score'] = dict(zip(self.X.columns, f_selector.scores_))
        return X_selected, selected_features

def calculate_metrics(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    conf_matrix = confusion_matrix(y_true, y_pred)
    class_report = classification_report(y_true, y_pred)

    print(f"\n{model_name} Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print("\nClassification Report:")
    print(class_report)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': conf_matrix,
        'classification_report': class_report
    }

def train_and_evaluate_models(X_train, X_test, y_train, y_test):
    models = {
        'KNN (From Scratch)': KNNFromScratch(k=3),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'Random Forest': RandomForestClassifier(random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42),
        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),
        'Naive Bayes': GaussianNB()
    }

    results = {}
    for name, model in models.items():
        print(f"\nTraining {name}...")
        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        start_time = time.time()
        y_pred = model.predict(X_test)
        predict_time = time.time() - start_time

        metrics = calculate_metrics(y_test, y_pred, name)
        metrics['train_time'] = train_time
        metrics['predict_time'] = predict_time
        results[name] = metrics

        print(f"Training time: {train_time:.2f} seconds")
        print(f"Prediction time: {predict_time:.2f} seconds")

    return results

def neural_network_tensorflow(X_train, X_test, y_train, y_test):
    num_classes = len(np.unique(y_train))
    if num_classes > 2:
        y_train_encoded = tf.keras.utils.to_categorical(y_train)
        y_test_encoded = tf.keras.utils.to_categorical(y_test)
        output_units = num_classes
        activation = 'softmax'
    else:
        y_train_encoded = y_train
        y_test_encoded = y_test
        output_units = 1
        activation = 'sigmoid'

    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(output_units, activation=activation)
    ])

    model.compile(optimizer='adam',
                 loss='binary_crossentropy' if num_classes == 2 else 'categorical_crossentropy',
                 metrics=['accuracy'])

    print("\nTraining Neural Network (TensorFlow)...")
    history = model.fit(X_train, y_train_encoded,
                       epochs=50,
                       batch_size=32,
                       validation_split=0.2,
                       verbose=1)

    y_pred_proba = model.predict(X_test)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten() if num_classes == 2 else np.argmax(y_pred_proba, axis=1)

    metrics = calculate_metrics(y_test, y_pred, "Neural Network (TensorFlow)")

    return model, history, metrics

def run_complete_pipeline(data_path, target_column='Status'):
    print(f"Loading data from {data_path}...")
    df = pd.read_csv(data_path)

    print("\nInitial data info:")
    print(df.info())
    print("\nTarget variable statistics:")
    print(df[target_column].describe())

    preprocessor = DataPreprocessor()
    df_processed = preprocessor.preprocess_data(df)

    X = df_processed.drop(columns=[target_column])
    y = df_processed[target_column]

    le = LabelEncoder()
    y = le.fit_transform(y)

    print("\nPerforming feature selection...")
    fs = FeatureSelection(X, y)
    X_corr = fs.correlation_filter()
    X_mi, mi_features = fs.mutual_information()
    X_chi, chi_features = fs.chi_square_selection()
    X_f, f_features = fs.f_score_selection()

    common_features = list(set(mi_features) & set(chi_features) & set(f_features))
    X_selected = X[common_features]

    print(f"\nSelected {len(common_features)} features: {common_features}")

    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nTraining data shape: {X_train.shape}")
    print(f"Testing data shape: {X_test.shape}")

    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)

    nn_model, nn_history, nn_metrics = neural_network_tensorflow(X_train, X_test, y_train, y_test)
    results['TensorFlow Neural Network'] = nn_metrics

    return {
        'feature_scores': fs.feature_scores,
        'selected_features': common_features,
        'model_results': results
    }

if __name__ == "__main__":
    data_path = '/content/drive/MyDrive/PA HW/Breast_Cancer_dataset.csv'
    results = run_complete_pipeline(data_path)
    print("\nFinal Results Summary:")
    print("="*50)
    for model_name, metrics in results['model_results'].items():
        print(f"\n{model_name}:")
        print("-"*30)
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall: {metrics['recall']:.4f}")
        print(f"F1 Score: {metrics['f1_score']:.4f}")
        if 'train_time' in metrics:
            print(f"Training Time: {metrics['train_time']:.2f} seconds")
            print(f"Prediction Time: {metrics['predict_time']:.2f} seconds")
    print("\nFeature Importance Summary:")
    print("="*50)
    for feature_type, scores in results['feature_scores'].items():
        print(f"\n{feature_type.upper()} Scores:")
        print("-"*30)
        sorted_features = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        for feature, score in sorted_features:
            print(f"{feature}: {score:.4f}")

    print("\nSelected Features:")
    print("-"*30)
    print(results['selected_features'])

    print("""
1. KNN (Implemented from scratch, no in-built libraries):
   - Summary: K-Nearest Neighbors is a simple, instance-based learning algorithm where predictions are made based on the majority label of the nearest k data points.
   - Pros: Easy to implement, intuitive, non-parametric.
   - Cons: Computationally expensive during prediction, sensitive to irrelevant features.
   - Hyperparameters:
     - k (number of neighbors)
     - Distance metric (e.g., Euclidean, Manhattan)
     - Weighting function (uniform or distance-based)

2. Naïve Bayes:
   - Summary: A probabilistic classifier based on Bayes' Theorem, assuming that features are conditionally independent given the class label.
   - Pros: Simple, fast, works well for text classification.
   - Cons: Assumes independence between features, which is often unrealistic.
   - Hyperparameters:
     - Prior probability for each class
     - Smoothing parameter (Laplace or Lidstone)

3. C4.5 Decision Tree:
   - Summary: A decision tree algorithm that builds a tree by recursively partitioning the data using information gain or gain ratio to choose the best splits.
   - Pros: Easy to interpret, handles both numerical and categorical data.
   - Cons: Can overfit, sensitive to noisy data.
   - Hyperparameters:
     - Maximum depth
     - Minimum samples per leaf
     - Pruning strategy (post-pruning or pre-pruning)

4. Random Forest:
   - Summary: An ensemble method that creates a forest of decision trees, each trained on a random subset of data and features, and combines their predictions.
   - Pros: Reduces overfitting, robust to noisy data.
   - Cons: Can be slow for large datasets, less interpretable.
   - Hyperparameters:
     - Number of trees
     - Maximum depth of trees
     - Minimum samples split

5. Gradient Boosting:
   - Summary: An ensemble method that builds trees sequentially, where each new tree corrects the errors of the previous one using gradient descent.
   - Pros: High predictive accuracy, handles different types of data.
   - Cons: Can overfit, slow to train, sensitive to hyperparameters.
   - Hyperparameters:
     - Learning rate
     - Number of estimators (trees)
     - Maximum depth of individual trees

6. Neural Networks:
   - Summary: A deep learning model that consists of layers of interconnected nodes (neurons), designed to learn complex patterns from data.
   - Pros: Can model highly complex relationships, flexible architecture.
   - Cons: Requires large amounts of data, computationally expensive, prone to overfitting without proper regularization.
   - Hyperparameters:
     - Number of layers
     - Number of neurons per layer
     - Learning rate
     - Batch size
     - Activation function
""")

def preprocess_data(df):
    df_processed = df.copy()
    label_encoders = {}
    categorical_columns = df_processed.select_dtypes(include=['object', 'category']).columns
    for column in categorical_columns:
        label_encoders[column] = LabelEncoder()
        df_processed[column] = label_encoders[column].fit_transform(df_processed[column].astype(str))

    for column in df_processed.columns:
        df_processed[column] = pd.to_numeric(df_processed[column], errors='coerce')
    df_processed = df_processed.fillna(df_processed.mean())

    scaler = StandardScaler()
    df_processed = pd.DataFrame(scaler.fit_transform(df_processed), columns=df_processed.columns)

    return df_processed

def step3_hyperparameter_search(X_train, X_test, y_train, y_test):
    print("\nStep 3: Hyperparameter Tuning with Random Search")
    print("="*50)
    rf_params = {
        'n_estimators': randint(50, 300),
        'max_depth': randint(5, 30),
        'min_samples_split': randint(2, 20),
        'min_samples_leaf': randint(1, 10)
    }
    gb_params = {
        'n_estimators': randint(50, 300),
        'learning_rate': uniform(0.01, 0.3),
        'max_depth': randint(3, 10),
        'min_samples_split': randint(2, 20)
    }
    rf_model = RandomForestClassifier(random_state=42)
    gb_model = GradientBoostingClassifier(random_state=42)

    rf_random = RandomizedSearchCV(
        rf_model, rf_params, n_iter=20, cv=5,
        scoring='f1_weighted', random_state=42, n_jobs=-1
    )
    gb_random = RandomizedSearchCV(
        gb_model, gb_params, n_iter=20, cv=5,
        scoring='f1_weighted', random_state=42, n_jobs=-1
    )

    print("\n1. Random Forest Hyperparameter Tuning:")
    print("-"*40)
    rf_random.fit(X_train, y_train)
    rf_pred = rf_random.predict(X_test)

    print("Best Parameters:", rf_random.best_params_)
    print("\nPerformance Metrics:")
    print(f"Accuracy: {accuracy_score(y_test, rf_pred):.4f}")
    print(f"Precision: {precision_score(y_test, rf_pred, average='weighted'):.4f}")
    print(f"Recall: {recall_score(y_test, rf_pred, average='weighted'):.4f}")
    print(f"F1 Score: {f1_score(y_test, rf_pred, average='weighted'):.4f}")
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, rf_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, rf_pred))
    print("\n2. Gradient Boosting Hyperparameter Tuning:")
    print("-"*40)
    gb_random.fit(X_train, y_train)
    gb_pred = gb_random.predict(X_test)

    print("Best Parameters:", gb_random.best_params_)
    print("\nPerformance Metrics:")
    print(f"Accuracy: {accuracy_score(y_test, gb_pred):.4f}")
    print(f"Precision: {precision_score(y_test, gb_pred, average='weighted'):.4f}")
    print(f"Recall: {recall_score(y_test, gb_pred, average='weighted'):.4f}")
    print(f"F1 Score: {f1_score(y_test, gb_pred, average='weighted'):.4f}")
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, gb_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, gb_pred))

    rf_f1 = f1_score(y_test, rf_pred, average='weighted')
    gb_f1 = f1_score(y_test, gb_pred, average='weighted')

    print("\nConclusion:")
    print("-"*40)
    if rf_f1 > gb_f1:
        print("Random Forest performed better with tuned parameters")
        print(f"Best F1 Score: {rf_f1:.4f}")
        print("Optimal parameters:", rf_random.best_params_)
    else:
        print("Gradient Boosting performed better with tuned parameters")
        print(f"Best F1 Score: {gb_f1:.4f}")
        print("Optimal parameters:", gb_random.best_params_)

if __name__ == "__main__":
    data_path = '/content/drive/MyDrive/PA HW/Breast_Cancer_dataset.csv'
    df = pd.read_csv(data_path)

    X = df.drop('Status', axis=1)
    y = df['Status']
    X_processed = preprocess_data(X)

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )

    step3_hyperparameter_search(X_train, X_test, y_train, y_test)